{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "                                                text  label\n",
      "0                            i didnt feel humiliated      0\n",
      "1  i can go from feeling so hopeless to so damned...      0\n",
      "2   im grabbing a minute to post i feel greedy wrong      3\n",
      "3  i am ever feeling nostalgic about the fireplac...      2\n",
      "4                               i am feeling grouchy      3\n",
      "\n",
      "Validation set:\n",
      "                                                text  label\n",
      "0  im feeling quite sad and sorry for myself but ...      0\n",
      "1  i feel like i am still looking at a blank canv...      0\n",
      "2                     i feel like a faithful servant      2\n",
      "3                  i am just feeling cranky and blue      3\n",
      "4  i can have for a treat or if i am feeling festive      1\n",
      "\n",
      "Test set:\n",
      "                                                text  label\n",
      "0  im feeling rather rotten so im not very ambiti...      0\n",
      "1          im updating my blog because i feel shitty      0\n",
      "2  i never make her separate from me because i do...      0\n",
      "3  i left with my bouquet of red and yellow tulip...      1\n",
      "4    i was feeling a little vain when i did this one      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f11a2b2d39d4b678346661ac558ea75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6547, 'grad_norm': 1.771509051322937, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.4}\n",
      "{'loss': 1.4732, 'grad_norm': 4.145961284637451, 'learning_rate': 7.333333333333333e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432b74b8160040899c8e5debbc0367a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.045021414756775, 'eval_accuracy': 0.64, 'eval_f1': 0.5135115864527628, 'eval_runtime': 53.802, 'eval_samples_per_second': 1.859, 'eval_steps_per_second': 0.074, 'epoch': 1.0}\n",
      "{'loss': 1.1747, 'grad_norm': 4.593270301818848, 'learning_rate': 6e-05, 'epoch': 1.2}\n",
      "{'loss': 1.053, 'grad_norm': 11.470431327819824, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.6}\n",
      "{'loss': 0.8536, 'grad_norm': 5.1431989669799805, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6269853acd42879c2d9fe21f327974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.713198184967041, 'eval_accuracy': 0.79, 'eval_f1': 0.7525587027914614, 'eval_runtime': 42.0753, 'eval_samples_per_second': 2.377, 'eval_steps_per_second': 0.095, 'epoch': 2.0}\n",
      "{'loss': 0.6229, 'grad_norm': 2.853569269180298, 'learning_rate': 2e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6304, 'grad_norm': 3.51495623588562, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18ea5ea1db341198e61d0108e93e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.630265474319458, 'eval_accuracy': 0.82, 'eval_f1': 0.8105693804403483, 'eval_runtime': 44.4288, 'eval_samples_per_second': 2.251, 'eval_steps_per_second': 0.09, 'epoch': 3.0}\n",
      "{'train_runtime': 3493.0857, 'train_samples_per_second': 0.687, 'train_steps_per_second': 0.021, 'train_loss': 1.033127899169922, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74582db53684c3a95e9955d5c1084fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation results:\n",
      "{'eval_loss': 0.630265474319458, 'eval_accuracy': 0.82, 'eval_f1': 0.8105693804403483, 'eval_runtime': 43.6794, 'eval_samples_per_second': 2.289, 'eval_steps_per_second': 0.092, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df5d762d06d4b8590120850f6e970d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results:\n",
      "{'eval_loss': 0.8035635948181152, 'eval_accuracy': 0.71, 'eval_f1': 0.6939156684911834, 'eval_runtime': 45.305, 'eval_samples_per_second': 2.207, 'eval_steps_per_second': 0.088, 'epoch': 3.0}\n",
      "\n",
      "Predictions:\n",
      "[[{'label': 'LABEL_0', 'score': 0.7109132409095764}, {'label': 'LABEL_1', 'score': 0.02293119952082634}, {'label': 'LABEL_2', 'score': 0.029121406376361847}, {'label': 'LABEL_3', 'score': 0.11977823078632355}, {'label': 'LABEL_4', 'score': 0.09192276000976562}, {'label': 'LABEL_5', 'score': 0.025333072990179062}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, AutoTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò (Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏Ïö©, ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_validation = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Train set:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(df_validation.head())\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(df_test.head())\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú Î∞è Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Î•º 40Î∂ÑÏùò 1Î°ú Ï§ÑÏûÑ\n",
    "train_size = len(tokenized_datasets['train']) // 20\n",
    "validation_size = len(tokenized_datasets['validation']) // 20\n",
    "test_size = len(tokenized_datasets['test']) // 20\n",
    "\n",
    "small_train_dataset = tokenized_datasets['train'].select(range(train_size))\n",
    "small_validation_dataset = tokenized_datasets['validation'].select(range(validation_size))\n",
    "small_test_dataset = tokenized_datasets['test'].select(range(test_size))\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
    "\n",
    "# Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞ Ìï®Ïàò Ï†ïÏùò\n",
    "def compute_metrics(p):\n",
    "    import numpy as np\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    f1 = f1_score(p.label_ids, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# ÌõàÎ†® Ïù∏Ïûê ÏÑ§Ï†ï\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Ìä∏Î†àÏù¥ÎÑà ÏÑ§Ï†ï Î∞è Ï°∞Í∏∞ Ï¢ÖÎ£å ÏΩúÎ∞± Ï∂îÍ∞Ä\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_validation_dataset,\n",
    "    compute_metrics=compute_metrics,  # Î©îÌä∏Î¶≠ Ìï®Ïàò Ï∂îÍ∞Ä\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "validation_results = trainer.evaluate()\n",
    "print(\"\\nValidation results:\")\n",
    "print(validation_results)\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=small_test_dataset)\n",
    "print(\"\\nTest results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Fine-tuning ÌõÑ Î™®Îç∏ÏùÑ Ï†ÄÏû•\n",
    "model.save_pretrained(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")\n",
    "\n",
    "# ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∞êÏ†ï Î∂ÑÎ•ò\n",
    "from transformers import pipeline\n",
    "\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"./results\", tokenizer=\"./results\", return_all_scores=True)\n",
    "\n",
    "# ÏòàÏãú ÌÖçÏä§Ìä∏Ïóê ÎåÄÌïú Í∞êÏ†ï Î∂ÑÎ•ò\n",
    "example_text = \"im feeling quite sad and sorry for myself but ill snap out of it soon\"\n",
    "predictions = emotion_classifier(example_text)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.9979704022407532}, {'label': 'LABEL_1', 'score': 0.0004122512764297426}, {'label': 'LABEL_2', 'score': 0.0005707133677788079}, {'label': 'LABEL_3', 'score': 0.00036665392690338194}, {'label': 'LABEL_4', 'score': 0.0002818174834828824}, {'label': 'LABEL_5', 'score': 0.00039819441735744476}], [{'label': 'LABEL_0', 'score': 0.9980321526527405}, {'label': 'LABEL_1', 'score': 0.0003956361033488065}, {'label': 'LABEL_2', 'score': 0.0005570072680711746}, {'label': 'LABEL_3', 'score': 0.0003561170888133347}, {'label': 'LABEL_4', 'score': 0.00027176193543709815}, {'label': 'LABEL_5', 'score': 0.0003873667155858129}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Î∂ÑÎ•òÍ∏∞ÏôÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "classifier = pipeline(\"text-classification\", model='./results', tokenizer='./results', return_all_scores=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results\")\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏Î•º ÎÇòÎàÑÎäî Ìï®Ïàò\n",
    "def chunk_text(text, tokenizer, chunk_size=510):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏Î•º ÎÇòÎà† Ï≤òÎ¶¨ÌïòÍ≥† Í≤∞Í≥ºÎ•º Í≤∞Ìï©ÌïòÎäî Ìï®Ïàò\n",
    "def classify_long_text(text, classifier, tokenizer):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    all_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        prediction = classifier(chunk)\n",
    "        all_scores.extend(prediction)  # Í∞Å Ï≤≠ÌÅ¨Ïùò ÏòàÏ∏°ÏùÑ Í≤∞Ìï©\n",
    "    \n",
    "    return all_scores\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏\n",
    "lyrics = \"\"\"I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two Yeah, I, I know it's hard to remember The people we used to be It's even harder to picture That you're not here next to me You said it's too late to make it But is it too late to try? And then that time that you wasted All of our bridges burned down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick You turned your back on tomorrow Cause you forgot yesterday I gave you my love to borrow But you just gave it away You can't expect me to be a fine I don't expect you to care I know I said it before But all of our bridges burnt down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone Man fuck that shit I'll be right here spending all this money while you sitting round Wondering why wasn't you who came out from nothing Made it from the botton Now when you see me I'm struting And all of my cause a way to push up a button Telling me the chances I blew up or whatever you call it Switched the number to my phone So you never can call it Don't need my name, or my show You can tell it I'm ballin' Shish, what a shame coulda got picked Had a really good game but you missed your last shot So you talk about who you see at the top Or what you could've saw But sad to say it's over for it Phantom roll out valet open doors Where's the car way, got what you was looking for Now ask me who they want So you can go take that little piece of shit with you I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all these fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone\"\"\"\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏ Î∂ÑÎ•ò Î∞è Í≤∞Í≥º Ï∂úÎ†•\n",
    "predictions = classify_long_text(lyrics, classifier, tokenizer)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.6237108707427979, 'joy': 0.3251424548216164, 'love': 0.01956860619247891, 'anger': 0.01791606517508626, 'fear': 0.01211931649595499, 'surprise': 0.0015426334575749934}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "model_name = \"bhadresh-savani/bert-base-uncased-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏Î•º ÎÇòÎàÑÎäî Ìï®Ïàò\n",
    "def chunk_text(text, tokenizer, chunk_size=510):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏Î•º ÎÇòÎà† Ï≤òÎ¶¨ÌïòÍ≥† Í≤∞Í≥ºÎ•º Í≤∞Ìï©ÌïòÎäî Ìï®Ïàò\n",
    "def classify_long_text(text, classifier, tokenizer):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    all_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prediction = classifier(chunk)\n",
    "        all_scores.extend(prediction)  # Í∞Å Ï≤≠ÌÅ¨Ïùò ÏòàÏ∏°ÏùÑ Í≤∞Ìï©\n",
    "    \n",
    "    # Í≤∞Í≥ºÎ•º Í∞êÏ†ïÎ≥ÑÎ°ú Ìï©ÏπòÍ∏∞\n",
    "    final_scores = {}\n",
    "    chunk_count = {label['label']: 0 for label in all_scores[0]}\n",
    "\n",
    "    for score_list in all_scores:\n",
    "        for score in score_list:\n",
    "            label = score['label']\n",
    "            if label not in final_scores:\n",
    "                final_scores[label] = 0\n",
    "            final_scores[label] += score['score']\n",
    "            chunk_count[label] += 1\n",
    "\n",
    "    # Ï¥ù Ï†êÏàòÎ•º ÌèâÍ∑† Ï†êÏàòÎ°ú Î≥ÄÌôò\n",
    "    for label in final_scores:\n",
    "        final_scores[label] /= chunk_count[label]\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏\n",
    "lyrics = \"\"\"I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two Yeah, I, I know it's hard to remember The people we used to be It's even harder to picture That you're not here next to me You said it's too late to make it But is it too late to try? And then that time that you wasted All of our bridges burned down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick You turned your back on tomorrow Cause you forgot yesterday I gave you my love to borrow But you just gave it away You can't expect me to be a fine I don't expect you to care I know I said it before But all of our bridges burnt down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone Man fuck that shit I'll be right here spending all this money while you sitting round Wondering why wasn't you who came out from nothing Made it from the botton Now when you see me I'm struting And all of my cause a way to push up a button Telling me the chances I blew up or whatever you call it Switched the number to my phone So you never can call it Don't need my name, or my show You can tell it I'm ballin' Shish, what a shame coulda got picked Had a really good game but you missed your last shot So you talk about who you see at the top Or what you could've saw But sad to say it's over for it Phantom roll out valet open doors Where's the car way, got what you was looking for Now ask me who they want So you can go take that little piece of shit with you I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all these fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone\"\"\"\n",
    "\n",
    "# ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∞êÏ†ï Î∂ÑÎ•ò\n",
    "emotion_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# Í∏¥ ÌÖçÏä§Ìä∏ Î∂ÑÎ•ò Î∞è Í≤∞Í≥º Ï∂úÎ†•\n",
    "predictions = classify_long_text(lyrics, emotion_classifier, tokenizer)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
