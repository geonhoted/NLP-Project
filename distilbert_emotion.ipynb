{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "                                                text  label\n",
      "0                            i didnt feel humiliated      0\n",
      "1  i can go from feeling so hopeless to so damned...      0\n",
      "2   im grabbing a minute to post i feel greedy wrong      3\n",
      "3  i am ever feeling nostalgic about the fireplac...      2\n",
      "4                               i am feeling grouchy      3\n",
      "\n",
      "Validation set:\n",
      "                                                text  label\n",
      "0  im feeling quite sad and sorry for myself but ...      0\n",
      "1  i feel like i am still looking at a blank canv...      0\n",
      "2                     i feel like a faithful servant      2\n",
      "3                  i am just feeling cranky and blue      3\n",
      "4  i can have for a treat or if i am feeling festive      1\n",
      "\n",
      "Test set:\n",
      "                                                text  label\n",
      "0  im feeling rather rotten so im not very ambiti...      0\n",
      "1          im updating my blog because i feel shitty      0\n",
      "2  i never make her separate from me because i do...      0\n",
      "3  i left with my bouquet of red and yellow tulip...      1\n",
      "4    i was feeling a little vain when i did this one      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f11a2b2d39d4b678346661ac558ea75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6547, 'grad_norm': 1.771509051322937, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.4}\n",
      "{'loss': 1.4732, 'grad_norm': 4.145961284637451, 'learning_rate': 7.333333333333333e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432b74b8160040899c8e5debbc0367a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.045021414756775, 'eval_accuracy': 0.64, 'eval_f1': 0.5135115864527628, 'eval_runtime': 53.802, 'eval_samples_per_second': 1.859, 'eval_steps_per_second': 0.074, 'epoch': 1.0}\n",
      "{'loss': 1.1747, 'grad_norm': 4.593270301818848, 'learning_rate': 6e-05, 'epoch': 1.2}\n",
      "{'loss': 1.053, 'grad_norm': 11.470431327819824, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.6}\n",
      "{'loss': 0.8536, 'grad_norm': 5.1431989669799805, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6269853acd42879c2d9fe21f327974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.713198184967041, 'eval_accuracy': 0.79, 'eval_f1': 0.7525587027914614, 'eval_runtime': 42.0753, 'eval_samples_per_second': 2.377, 'eval_steps_per_second': 0.095, 'epoch': 2.0}\n",
      "{'loss': 0.6229, 'grad_norm': 2.853569269180298, 'learning_rate': 2e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6304, 'grad_norm': 3.51495623588562, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18ea5ea1db341198e61d0108e93e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.630265474319458, 'eval_accuracy': 0.82, 'eval_f1': 0.8105693804403483, 'eval_runtime': 44.4288, 'eval_samples_per_second': 2.251, 'eval_steps_per_second': 0.09, 'epoch': 3.0}\n",
      "{'train_runtime': 3493.0857, 'train_samples_per_second': 0.687, 'train_steps_per_second': 0.021, 'train_loss': 1.033127899169922, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74582db53684c3a95e9955d5c1084fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation results:\n",
      "{'eval_loss': 0.630265474319458, 'eval_accuracy': 0.82, 'eval_f1': 0.8105693804403483, 'eval_runtime': 43.6794, 'eval_samples_per_second': 2.289, 'eval_steps_per_second': 0.092, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df5d762d06d4b8590120850f6e970d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results:\n",
      "{'eval_loss': 0.8035635948181152, 'eval_accuracy': 0.71, 'eval_f1': 0.6939156684911834, 'eval_runtime': 45.305, 'eval_samples_per_second': 2.207, 'eval_steps_per_second': 0.088, 'epoch': 3.0}\n",
      "\n",
      "Predictions:\n",
      "[[{'label': 'LABEL_0', 'score': 0.7109132409095764}, {'label': 'LABEL_1', 'score': 0.02293119952082634}, {'label': 'LABEL_2', 'score': 0.029121406376361847}, {'label': 'LABEL_3', 'score': 0.11977823078632355}, {'label': 'LABEL_4', 'score': 0.09192276000976562}, {'label': 'LABEL_5', 'score': 0.025333072990179062}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, AutoTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# 데이터프레임으로 변환 (데이터 확인용, 선택사항)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_validation = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Train set:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(df_validation.head())\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(df_test.head())\n",
    "\n",
    "# 토크나이저 로드 및 데이터 전처리\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 데이터셋 크기를 40분의 1로 줄임\n",
    "train_size = len(tokenized_datasets['train']) // 20\n",
    "validation_size = len(tokenized_datasets['validation']) // 20\n",
    "test_size = len(tokenized_datasets['test']) // 20\n",
    "\n",
    "small_train_dataset = tokenized_datasets['train'].select(range(train_size))\n",
    "small_validation_dataset = tokenized_datasets['validation'].select(range(validation_size))\n",
    "small_test_dataset = tokenized_datasets['test'].select(range(test_size))\n",
    "\n",
    "# 모델 로드\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
    "\n",
    "# 메트릭 계산 함수 정의\n",
    "def compute_metrics(p):\n",
    "    import numpy as np\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    f1 = f1_score(p.label_ids, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# 트레이너 설정 및 조기 종료 콜백 추가\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_validation_dataset,\n",
    "    compute_metrics=compute_metrics,  # 메트릭 함수 추가\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "validation_results = trainer.evaluate()\n",
    "print(\"\\nValidation results:\")\n",
    "print(validation_results)\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=small_test_dataset)\n",
    "print(\"\\nTest results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Fine-tuning 후 모델을 저장\n",
    "model.save_pretrained(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")\n",
    "\n",
    "# 파이프라인을 사용하여 감정 분류\n",
    "from transformers import pipeline\n",
    "\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"./results\", tokenizer=\"./results\", return_all_scores=True)\n",
    "\n",
    "# 예시 텍스트에 대한 감정 분류\n",
    "example_text = \"im feeling quite sad and sorry for myself but ill snap out of it soon\"\n",
    "predictions = emotion_classifier(example_text)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.9979704022407532}, {'label': 'LABEL_1', 'score': 0.0004122512764297426}, {'label': 'LABEL_2', 'score': 0.0005707133677788079}, {'label': 'LABEL_3', 'score': 0.00036665392690338194}, {'label': 'LABEL_4', 'score': 0.0002818174834828824}, {'label': 'LABEL_5', 'score': 0.00039819441735744476}], [{'label': 'LABEL_0', 'score': 0.9980321526527405}, {'label': 'LABEL_1', 'score': 0.0003956361033488065}, {'label': 'LABEL_2', 'score': 0.0005570072680711746}, {'label': 'LABEL_3', 'score': 0.0003561170888133347}, {'label': 'LABEL_4', 'score': 0.00027176193543709815}, {'label': 'LABEL_5', 'score': 0.0003873667155858129}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# 분류기와 토크나이저 로드\n",
    "classifier = pipeline(\"text-classification\", model='./results', tokenizer='./results', return_all_scores=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results\")\n",
    "\n",
    "# 긴 텍스트를 나누는 함수\n",
    "def chunk_text(text, tokenizer, chunk_size=510):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "# 긴 텍스트를 나눠 처리하고 결과를 결합하는 함수\n",
    "def classify_long_text(text, classifier, tokenizer):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    all_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        prediction = classifier(chunk)\n",
    "        all_scores.extend(prediction)  # 각 청크의 예측을 결합\n",
    "    \n",
    "    return all_scores\n",
    "\n",
    "# 긴 텍스트\n",
    "lyrics = \"\"\"I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two Yeah, I, I know it's hard to remember The people we used to be It's even harder to picture That you're not here next to me You said it's too late to make it But is it too late to try? And then that time that you wasted All of our bridges burned down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick You turned your back on tomorrow Cause you forgot yesterday I gave you my love to borrow But you just gave it away You can't expect me to be a fine I don't expect you to care I know I said it before But all of our bridges burnt down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone Man fuck that shit I'll be right here spending all this money while you sitting round Wondering why wasn't you who came out from nothing Made it from the botton Now when you see me I'm struting And all of my cause a way to push up a button Telling me the chances I blew up or whatever you call it Switched the number to my phone So you never can call it Don't need my name, or my show You can tell it I'm ballin' Shish, what a shame coulda got picked Had a really good game but you missed your last shot So you talk about who you see at the top Or what you could've saw But sad to say it's over for it Phantom roll out valet open doors Where's the car way, got what you was looking for Now ask me who they want So you can go take that little piece of shit with you I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all these fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone\"\"\"\n",
    "\n",
    "# 긴 텍스트 분류 및 결과 출력\n",
    "predictions = classify_long_text(lyrics, classifier, tokenizer)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.6237108707427979, 'joy': 0.3251424548216164, 'love': 0.01956860619247891, 'anger': 0.01791606517508626, 'fear': 0.01211931649595499, 'surprise': 0.0015426334575749934}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"bhadresh-savani/bert-base-uncased-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 긴 텍스트를 나누는 함수\n",
    "def chunk_text(text, tokenizer, chunk_size=510):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "# 긴 텍스트를 나눠 처리하고 결과를 결합하는 함수\n",
    "def classify_long_text(text, classifier, tokenizer):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    all_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prediction = classifier(chunk)\n",
    "        all_scores.extend(prediction)  # 각 청크의 예측을 결합\n",
    "    \n",
    "    # 결과를 감정별로 합치기\n",
    "    final_scores = {}\n",
    "    chunk_count = {label['label']: 0 for label in all_scores[0]}\n",
    "\n",
    "    for score_list in all_scores:\n",
    "        for score in score_list:\n",
    "            label = score['label']\n",
    "            if label not in final_scores:\n",
    "                final_scores[label] = 0\n",
    "            final_scores[label] += score['score']\n",
    "            chunk_count[label] += 1\n",
    "\n",
    "    # 총 점수를 평균 점수로 변환\n",
    "    for label in final_scores:\n",
    "        final_scores[label] /= chunk_count[label]\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "# 긴 텍스트\n",
    "lyrics = \"\"\"I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two Yeah, I, I know it's hard to remember The people we used to be It's even harder to picture That you're not here next to me You said it's too late to make it But is it too late to try? And then that time that you wasted All of our bridges burned down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick You turned your back on tomorrow Cause you forgot yesterday I gave you my love to borrow But you just gave it away You can't expect me to be a fine I don't expect you to care I know I said it before But all of our bridges burnt down I've wasted my nights You turned out the lights Now I'm paralyzed Still stucked in that time when we called it love But even the sun sets in paradise I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all those fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone Man fuck that shit I'll be right here spending all this money while you sitting round Wondering why wasn't you who came out from nothing Made it from the botton Now when you see me I'm struting And all of my cause a way to push up a button Telling me the chances I blew up or whatever you call it Switched the number to my phone So you never can call it Don't need my name, or my show You can tell it I'm ballin' Shish, what a shame coulda got picked Had a really good game but you missed your last shot So you talk about who you see at the top Or what you could've saw But sad to say it's over for it Phantom roll out valet open doors Where's the car way, got what you was looking for Now ask me who they want So you can go take that little piece of shit with you I'm at a payphone trying to call home All of my change I've spent on you Where are the times gone baby It's all wrong, where are the plans we made for two If happy ever after did exist I would still be holding you like this And all these fairytales are full of shit One more fucking love song I'll be sick Now I'm at a payphone\"\"\"\n",
    "\n",
    "# 파이프라인을 사용하여 감정 분류\n",
    "emotion_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# 긴 텍스트 분류 및 결과 출력\n",
    "predictions = classify_long_text(lyrics, emotion_classifier, tokenizer)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
